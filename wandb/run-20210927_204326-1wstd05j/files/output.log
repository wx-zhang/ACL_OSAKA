
load from omniglot.npy.
Num parameters in Shared       = 13.10K
Num parameters in Private      = 37.68K,  per task = 12.56K
Num parameters in Header       = 4.21K,  per task = 1.41K
Num parameters in P+H          = 41.90K
-------------------------->   Total architecture size: 55.00K parameters (220.00KB)
Num parameters in D       = 578.0
1.6206926107406616 20.0 1.6211940050125122 20.0
1.6202203035354614 20.0 1.6217867136001587 20.0
1.618940830230713 20.0 1.6202558279037476 20.0
1.6176426410675049 20.0 1.6190568208694458 20.0
1.6154868602752686 20.0 1.6175451278686523 20.0
1.6133500337600708 20.0 1.6161130666732788 16.0
1.6106059551239014 20.0 1.6152054071426392 16.0
1.6083217859268188 24.0 1.6122541427612305 24.0
1.6046760082244873 29.333333333333332 1.604447841644287 28.0
1.5996614694595337 22.666666666666668 1.5998951196670532 24.0
1.647975206375122 20.0 1.6448537111282349 20.0
1.661805272102356 20.0 1.6592659950256348 20.0
 Dis lr=3.3e-03
1.6496164798736572 20.0 1.646104335784912 20.0
1.657918930053711 20.0 1.6602752208709717 20.0
1.6285381317138672 20.0 1.637992024421692 20.0
1.6061831712722778 20.0 1.6092759370803833 20.0
1.6087629795074463 20.0 1.6099549531936646 20.0
1.59701669216156 20.0 1.6020328998565674 20.0
1.584875464439392 20.0 1.5917444229125977 20.0
1.5748255252838135 22.666666666666668 1.584202527999878 24.0
1.5692631006240845 26.666666666666668 1.5815972089767456 28.0
1.5615307092666626 34.666666666666664 1.5772429704666138 32.0
 Dis lr=1.1e-03
1.5500353574752808 42.666666666666664 1.5700416564941406 36.0
1.5274028778076172 49.333333333333336 1.5578534603118896 36.0
1.5065349340438843 53.333333333333336 1.541858196258545 40.0
1.4852511882781982 53.333333333333336 1.5286427736282349 40.0
1.4596927165985107 53.333333333333336 1.5146706104278564 40.0
1.4298169612884521 54.666666666666664 1.5026307106018066 36.0
1.3966810703277588 54.666666666666664 1.4900459051132202 40.0
1.3585643768310547 54.666666666666664 1.4698891639709473 40.0
1.3179209232330322 53.333333333333336 1.4504772424697876 40.0
1.270565152168274 54.666666666666664 1.4290027618408203 40.0
 Dis lr=3.7e-04
1.2137740850448608 56.0 1.3975757360458374 40.0
1.1772515773773193 60.0 1.386359453201294 40.0
1.3753031492233276 37.333333333333336 1.6362106800079346 28.0
1.6310744285583496 20.0 1.8855061531066895 24.0
1.5865291357040405 22.666666666666668 1.8765079975128174 24.0
1.647213339805603 25.333333333333332 1.9416840076446533 24.0
1.3133931159973145 49.333333333333336 1.7163065671920776 24.0
1.0807244777679443 60.0 1.5350902080535889 44.0
0.8765456676483154 64.0 1.3494499921798706 44.0
0.8340188264846802 64.0 1.379392147064209 48.0
 Dis lr=1.2e-04
0.7938483953475952 65.33333333333333 1.3338974714279175 48.0
0.7217786312103271 61.333333333333336 1.2135827541351318 44.0
0.6974225640296936 68.0 1.310660719871521 48.0
0.6759913563728333 61.333333333333336 1.2932318449020386 52.0
0.7081075310707092 60.0 1.339490294456482 44.0
0.6852765679359436 61.333333333333336 1.3337393999099731 40.0
0.635475754737854 69.33333333333333 1.3525937795639038 40.0
0.6034900546073914 72.0 1.368212103843689 44.0
0.6531019806861877 62.666666666666664 1.3871688842773438 40.0
0.597628653049469 64.0 1.350149393081665 40.0
 Dis lr=4.1e-05
0.5280031561851501 76.0 1.2987252473831177 44.0
0.569359540939331 65.33333333333333 1.3354309797286987 48.0
 lr=3.3e-020.4909665584564209 84.0 1.2799046039581299 48.0
0.4861769378185272 97.33333333333333 1.3028870820999146 44.0
0.48055508732795715 97.33333333333333 1.2950876951217651 48.0
0.4746739864349365 98.66666666666667 1.3034014701843262 40.0
0.47830918431282043 86.66666666666667 1.3184733390808105 52.0
0.47122591733932495 88.0 1.3366219997406006 52.0
0.4517638683319092 94.66666666666667 1.3224568367004395 44.0
0.44132834672927856 96.0 1.3209973573684692 52.0
 Dis lr=1.4e-05
0.43760156631469727 94.66666666666667 1.32376229763031 52.0
0.4293239414691925 93.33333333333333 1.3308289051055908 52.0
 lr=1.1e-020.42629608511924744 94.66666666666667 1.3295273780822754 52.0
0.4254996180534363 94.66666666666667 1.3316524028778076 52.0
0.4243387281894684 94.66666666666667 1.3353853225708008 48.0
0.42025336623191833 94.66666666666667 1.338922142982483 48.0
0.42074546217918396 92.0 1.3499035835266113 52.0
0.4189862906932831 89.33333333333333 1.3555235862731934 52.0
0.4189312160015106 90.66666666666667 1.365681529045105 52.0
0.41561049222946167 90.66666666666667 1.3647990226745605 52.0
 Dis lr=4.6e-06
0.4142507016658783 90.66666666666667 1.361741304397583 52.0
0.40956979990005493 93.33333333333333 1.359965205192566 52.0
 lr=3.7e-030.4057997465133667 93.33333333333333 1.356454849243164 52.0
0.4029166102409363 97.33333333333333 1.3544045686721802 52.0
0.40134143829345703 97.33333333333333 1.3560203313827515 52.0
0.4004480242729187 97.33333333333333 1.3588306903839111 52.0
0.3983740210533142 98.66666666666667 1.3592218160629272 52.0
0.39605310559272766 98.66666666666667 1.3569523096084595 52.0
0.3940953016281128 100.0 1.357164740562439 48.0
0.3927697241306305 100.0 1.3588985204696655 48.0
 Dis lr=1.5e-06
0.39071816205978394 100.0 1.3592942953109741 48.0
0.3896707594394684 100.0 1.3603665828704834 48.0
 lr=1.2e-030.39045852422714233 100.0 1.360837697982788 48.0
0.3903917968273163 100.0 1.361649513244629 48.0
0.39047321677207947 100.0 1.3608577251434326 48.0
0.38967329263687134 100.0 1.3619003295898438 48.0
0.389968603849411 100.0 1.3634811639785767 48.0
0.38982093334198 100.0 1.3643991947174072 48.0
0.38987788558006287 100.0 1.3652888536453247 48.0
0.39002618193626404 100.0 1.365452527999878 48.0
 Dis lr=5.1e-07
Dis lr reached minimum value
Saving all models for task 1 ...
>>> Test on task meta - omniglot meta set: loss=1.888, acc_measure=24.00000000 <<<
Traceback (most recent call last):
  File "main.py", line 101, in <module>
    args.wandb and utils.log_wandb(test_res,0,'pre-train_test',wandb)
  File "/Users/wenxuanzhang/Desktop/ACL_OSAKA/utils.py", line 56, in log_wandb
    wandb.log({'Pre-train_test_task_loss': res['loss_t']})
AttributeError: 'NoneType' object has no attribute 'log'
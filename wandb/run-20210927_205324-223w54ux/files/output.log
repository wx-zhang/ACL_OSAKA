
load from omniglot.npy.
Num parameters in Shared       = 13.10K
Num parameters in Private      = 37.68K,  per task = 12.56K
Num parameters in Header       = 4.21K,  per task = 1.41K
Num parameters in P+H          = 41.90K
-------------------------->   Total architecture size: 55.00K parameters (220.00KB)
Num parameters in D       = 578.0
1.6013982772827149 25.666666666666668 1.6048856735229493 23.2
1.5806175231933595 31.133333333333333 1.5910198211669921 26.6
1.55761775970459 32.53333333333333 1.5829544067382812 25.2
1.5205320358276366 35.0 1.5683829307556152 26.6
1.5001539230346679 33.666666666666664 1.5757192611694335 25.2
1.4650442123413085 36.53333333333333 1.5775393486022948 27.0
1.4489175796508789 38.8 1.580434226989746 30.2
1.4325968742370605 40.6 1.5595027923583984 29.8
2.211871337890625 22.2 2.4128139495849608 21.0
1.3982609748840331 42.6 1.577549362182617 31.6
1.4273859977722168 40.93333333333333 1.6376968383789063 29.4
1.3437264442443848 45.13333333333333 1.6101823806762696 32.6
1.5412416458129883 33.46666666666667 1.8782817840576171 26.0
1.530728530883789 37.0 1.7675653457641602 29.6
1.3148182868957519 44.4 1.6274906158447267 31.2
1.4034718513488769 43.333333333333336 1.7362188339233398 31.6
1.3878396987915038 43.86666666666667 1.8110933303833008 30.0
1.2882506370544433 48.53333333333333 1.692535400390625 33.0
 lr=3.3e-021.1813599586486816 53.8 1.6336139678955077 32.6
1.1666135787963867 54.6 1.659994888305664 34.6
1.1621655464172362 54.266666666666666 1.683434295654297 33.6
1.1726154327392577 54.266666666666666 1.7324052810668946 32.0
1.1237314224243165 55.733333333333334 1.7072740554809571 34.2
1.1072943687438965 56.06666666666667 1.732585334777832 33.2
1.1388517379760743 54.6 1.751025390625 31.8
1.1936678886413574 52.46666666666667 1.860578727722168 33.2
1.164778995513916 54.93333333333333 1.9131450653076172 32.2
1.1848316192626953 54.2 1.8943294525146483 31.8
 lr=1.1e-021.080162239074707 57.333333333333336 1.785104751586914 33.8
1.0559749603271484 58.733333333333334 1.811577606201172 32.6
1.0610478401184082 58.46666666666667 1.8192880630493165 33.4
1.05386962890625 57.733333333333334 1.831336212158203 32.8
1.0457833290100098 58.4 1.8458906173706056 33.2
1.0470291137695313 59.53333333333333 1.8722322463989258 31.8
1.041707420349121 59.733333333333334 1.9118560791015624 32.4
1.0328349113464355 59.8 1.8946866989135742 32.4
1.040113925933838 59.0 1.9133955001831056 32.6
1.026725959777832 59.8 1.905506706237793 33.4
 lr=3.7e-031.0135284423828126 59.8 1.902625846862793 34.2
1.0153973579406739 59.86666666666667 1.9071765899658204 34.2
1.0159225463867188 59.266666666666666 1.9156003952026368 32.8
1.0115534782409668 59.666666666666664 1.9272865295410155 33.4
1.0068697929382324 60.06666666666667 1.9361183166503906 34.2
1.0021979331970214 60.46666666666667 1.9488943099975586 34.4
1.0040799140930177 60.2 1.943413543701172 34.0
1.0034823417663574 60.86666666666667 1.9524864196777343 34.2
1.0069750785827636 59.8 1.9547073364257812 33.8
1.0039295196533202 59.733333333333334 1.972479248046875 34.2
 lr=1.2e-031.0032053947448731 59.666666666666664 1.9692596435546874 33.6
1.0026122093200684 59.8 1.967608642578125 33.8
1.001668930053711 60.0 1.971843147277832 33.6
0.9984511375427246 60.2 1.9707107543945312 34.2
0.9977635383605957 59.93333333333333 1.9707654953002929 34.0
0.9990225791931152 59.93333333333333 1.972367286682129 33.6
0.9980206489562988 60.4 1.9750141143798827 33.8
0.9983331680297851 60.46666666666667 1.9784526824951172 34.2
0.9979126930236817 60.2 1.979022216796875 34.6
0.9958308219909668 60.4 1.9813772201538087 34.2
 lr=4.1e-040.9979114532470703 60.2 1.9840957641601562 33.6
0.9976826667785644 59.93333333333333 1.9832456588745118 34.0
0.9976235389709472 60.0 1.9845664978027344 33.6
0.9975440979003907 59.93333333333333 1.9860811233520508 33.6
0.9960143089294433 60.53333333333333 1.9856285095214843 33.6
0.9959345817565918 60.2 1.9871097564697267 33.8
0.9953439712524415 60.46666666666667 1.9868892669677733 34.0
0.9956867218017578 60.4 1.9877670288085938 33.8
0.9950819969177246 60.46666666666667 1.9894941329956055 33.6
0.9953871726989746 60.2 1.9896831512451172 33.6
 lr=1.4e-040.9951381683349609 60.266666666666666 1.9907398223876953 33.6
0.9952735900878906 60.333333333333336 1.9912460327148438 33.8
0.9951399803161621 60.4 1.9910037994384766 33.6
0.9953775405883789 60.4 1.9911180496215821 33.6
0.9951101303100586 60.266666666666666 1.9912919998168945 33.8
0.9949346542358398 60.46666666666667 1.991729736328125 33.6
0.9949786186218261 60.46666666666667 1.9921905517578125 33.8
0.9947538375854492 60.53333333333333 1.9925134658813477 33.8
0.994692611694336 60.4 1.9924888610839844 33.8
0.9947659492492675 60.4 1.9923324584960938 33.8
 lr=4.6e-050.9949613571166992 60.46666666666667 1.9925617218017577 33.8
0.9949342727661132 60.46666666666667 1.9925935745239258 33.8
0.9948506355285645 60.4 1.9926090240478516 33.8
0.9949076652526856 60.4 1.9927112579345703 33.8
0.994809341430664 60.46666666666667 1.9928976058959962 33.8
0.9948378562927246 60.46666666666667 1.9929931640625 33.8
0.9948455810546875 60.4 1.992978858947754 33.8
0.9948086738586426 60.4 1.9930738449096679 33.8
0.9947771072387696 60.333333333333336 1.9929807662963868 33.8
0.9947954177856445 60.4 1.9931446075439454 33.8
 lr=1.5e-050.9948440551757812 60.4 1.9931303024291993 33.8
0.9948273658752441 60.46666666666667 1.9931512832641602 33.8
0.994813346862793 60.4 1.993218231201172 33.8
0.9947978973388671 60.46666666666667 1.9931816101074218 33.8
0.9948152542114258 60.46666666666667 1.993229866027832 33.8
0.9947729110717773 60.4 1.9932317733764648 33.8
0.9947697639465332 60.4 1.9932842254638672 33.8
0.9947857856750488 60.4 1.9933229446411134 33.8
0.9947489738464356 60.4 1.9932937622070312 33.8
0.994725227355957 60.4 1.9933324813842774 33.8
 lr=5.1e-060.99476318359375 60.333333333333336 1.9933601379394532 33.8
0.9947591781616211 60.333333333333336 1.9933464050292968 33.8
Saving all models for task 1 ...
>>> Test on task meta - omniglot meta set: loss=1.761, acc_measure=6.00000000 <<<
pretraining done!
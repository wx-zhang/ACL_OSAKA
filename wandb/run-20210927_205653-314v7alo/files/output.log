
load from omniglot.npy.
Num parameters in Shared       = 13.10K
Num parameters in Private      = 37.68K,  per task = 12.56K
Num parameters in Header       = 4.21K,  per task = 1.41K
Num parameters in P+H          = 41.90K
-------------------------->   Total architecture size: 55.00K parameters (220.00KB)
Num parameters in D       = 578.0
1.6169820785522462 20.0 1.617181396484375 20.0
1.610354232788086 20.0 1.610361099243164 20.0
1.6104063034057616 20.0 1.6104001998901367 20.0
1.6102758407592774 20.0 1.610280418395996 20.0
1.6102411270141601 20.0 1.6102388381958008 20.0
1.610188865661621 20.0 1.6101856231689453 20.0
1.6101749420166016 20.0 1.6101762771606445 20.0
1.6101593017578124 20.0 1.6101638793945312 20.0
1.6101034164428711 20.0 1.6101099014282227 20.0
1.610025978088379 20.0 1.6100299835205079 20.0
1.6099750518798828 20.0 1.6099790573120116 20.0
1.6099126815795899 20.0 1.6099090576171875 20.0
1.6098915100097657 20.0 1.609888458251953 20.0
1.6098752975463868 20.0 1.6098772048950196 20.0
1.6098278045654297 20.0 1.6098257064819337 20.0
1.6098108291625977 20.0 1.6098129272460937 20.0
1.6097883224487304 20.0 1.6097909927368164 20.0
1.6097345352172852 20.0 1.6097341537475587 20.0
1.609694480895996 20.0 1.6096986770629882 20.0
1.609674072265625 20.0 1.6096784591674804 20.0
1.6096420288085938 20.0 1.6096446990966797 20.0
1.6096275329589844 20.0 1.6096307754516601 20.0
1.6096172332763672 20.0 1.6096220016479492 20.0
1.6096128463745116 20.0 1.6096168518066407 20.0
1.6095863342285157 20.0 1.6095897674560546 20.0
1.6095722198486329 20.0 1.6095746994018554 20.0
1.6095603942871093 20.0 1.6095645904541016 20.0
1.6095609664916992 20.0 1.6095624923706056 20.0
1.6095443725585938 20.0 1.6095476150512695 20.0
1.6095382690429687 20.0 1.6095390319824219 20.0
1.609527015686035 20.0 1.6095258712768554 20.0
1.6095251083374023 20.0 1.6095260620117187 20.0
1.6095199584960938 20.0 1.609522819519043 20.0
1.609503173828125 20.0 1.609506607055664 20.0
1.6094751358032227 20.0 1.6094789505004883 20.0
1.6094738006591798 20.0 1.6094783782958983 20.0
1.6094717025756835 20.0 1.6094768524169922 20.0
1.6094432830810548 20.0 1.6094512939453125 20.0
1.609444808959961 20.0 1.609452247619629 20.0
1.6094347000122071 20.0 1.6094444274902344 20.0
1.6094255447387695 20.0 1.609440231323242 20.0
1.6094257354736328 20.0 1.609445571899414 20.0
1.6093868255615233 20.333333333333332 1.6094135284423827 20.6
1.6093656539916992 20.0 1.6094041824340821 20.0
1.6092844009399414 20.066666666666666 1.6093503952026367 20.0
1.6096481323242187 20.066666666666666 1.6096527099609375 20.0
1.60953369140625 19.8 1.6095510482788087 19.6
1.6095129013061524 20.0 1.6095355987548827 20.0
1.6095720291137696 19.533333333333335 1.609590721130371 19.4
1.6095243453979493 19.666666666666668 1.6095481872558595 19.8
1.6094350814819336 19.933333333333334 1.6094636917114258 20.0
1.6096626281738282 19.866666666666667 1.609665870666504 20.0
1.609532356262207 20.0 1.6095664978027344 20.0
1.609574317932129 19.933333333333334 1.6096113204956055 20.0
1.6095041275024413 19.933333333333334 1.6095674514770508 20.0
 lr=3.3e-021.6095310211181642 19.933333333333334 1.6095863342285157 20.0
1.6095546722412108 19.8 1.6096118927001952 20.0
1.6094720840454102 19.933333333333334 1.6095518112182616 20.0
1.6094741821289062 19.866666666666667 1.6095590591430664 20.0
1.6094167709350586 19.933333333333334 1.6095117568969726 20.0
1.6093921661376953 19.933333333333334 1.6094926834106444 20.0
1.6095449447631835 19.666666666666668 1.6096157073974608 19.6
1.6094711303710938 19.8 1.609573745727539 20.0
1.6094655990600586 19.8 1.609586524963379 19.6
1.609581756591797 19.533333333333335 1.609678077697754 19.6
 lr=1.1e-021.6096311569213868 19.266666666666666 1.6097211837768555 19.4
1.6093963623046874 19.8 1.6095342636108398 19.6
1.6093938827514649 19.6 1.6095308303833007 19.6
1.6093862533569336 19.6 1.6095252990722657 19.4
1.6092897415161134 19.866666666666667 1.6094533920288085 20.0
1.6093988418579102 19.4 1.6095609664916992 19.8
1.6097108840942382 18.8 1.6097677230834961 18.2
1.6093456268310546 19.666666666666668 1.6095050811767577 19.4
1.609644317626953 18.933333333333334 1.6097097396850586 18.6
1.6094295501708984 19.266666666666666 1.6096017837524415 19.6
 lr=3.7e-031.6092731475830078 19.333333333333332 1.6094797134399415 19.6
1.6093339920043945 19.266666666666666 1.609520721435547 19.6
1.6092531204223632 19.4 1.609471893310547 19.6
1.6091636657714843 19.533333333333335 1.6093910217285157 19.4
1.6091148376464843 19.866666666666667 1.6093549728393555 19.4
1.6090126037597656 19.866666666666667 1.6092767715454102 19.4
1.6092247009277343 19.533333333333335 1.6094427108764648 19.6
1.6092117309570313 19.333333333333332 1.6094463348388672 19.6
1.6092655181884765 19.266666666666666 1.609492874145508 19.6
1.6091318130493164 19.466666666666665 1.6093679428100587 19.6
1.6091276168823243 19.466666666666665 1.609391212463379 19.8
1.6091875076293944 19.266666666666666 1.6094415664672852 19.6
1.6091011047363282 19.466666666666665 1.6093578338623047 19.6
1.6092374801635743 19.066666666666666 1.6094818115234375 19.6
1.6091835021972656 19.066666666666666 1.6094463348388672 19.6
1.6090938568115234 19.4 1.60937557220459 19.6
 lr=1.2e-031.609034538269043 19.533333333333335 1.6093297958374024 19.6
1.608929443359375 19.4 1.6092418670654296 19.6
1.608984375 19.4 1.609283447265625 19.6
1.6090635299682616 19.333333333333332 1.6093538284301758 19.6
1.6090143203735352 19.333333333333332 1.6093233108520508 19.6
1.609048843383789 19.266666666666666 1.6093446731567382 19.6
1.6089464187622071 19.333333333333332 1.6092782974243165 19.6
1.608983039855957 19.2 1.6093002319335938 19.6
1.6089767456054687 19.4 1.6092918395996094 19.8
Saving all models for task 1 ...
>>> Test on task meta - omniglot meta set: loss=1.609, acc_measure=20.00000000 <<<
pretraining done!